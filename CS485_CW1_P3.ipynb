{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VX2qLI9zRFA7"
   },
   "source": [
    "# LDA Ensemble for Face Recognition\n",
    "\n",
    "Use the provided face data, and the same data partition into training and testing as in Q1.\n",
    "\n",
    "Try PCA-LDA and its ensemble learning, along with the NN classifier. Compare and discuss face recognition results.\n",
    "\n",
    "## PCA-LDA Ensemble\n",
    "\n",
    "Perform the PCA-LDA based face recognition with the NN classifier. Report and discuss, including:\n",
    "\n",
    "\n",
    "*   Recognition acuracies by varying the parameter values, M_pca and M_lda\n",
    "*   Ranks of the scatter matrices\n",
    "*   Confusion matrix, example of success and failure cases\n",
    "\n",
    "Explain your observations and reasons, and discuss the results in comparison to those of Q1.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76PZI5JMRQke"
   },
   "source": [
    "1. Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "executionInfo": {
     "elapsed": 121694,
     "status": "error",
     "timestamp": 1730519368592,
     "user": {
      "displayName": "신지환",
      "userId": "04463358478114875480"
     },
     "user_tz": -540
    },
    "id": "9OwKOMngPPWX",
    "outputId": "ae5cb18c-8d17-4477-d57a-b6775ac029dd"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "mount failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-aba055661d51>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Import necessary libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         )\n\u001b[0;32m--> 277\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: mount failed"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as spio\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNezXcS2RZDP"
   },
   "source": [
    "2. Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 43,
     "status": "aborted",
     "timestamp": 1730519368600,
     "user": {
      "displayName": "신지환",
      "userId": "04463358478114875480"
     },
     "user_tz": -540
    },
    "id": "Ka22xXV5PUjZ"
   },
   "outputs": [],
   "source": [
    "face_mat = spio.loadmat('/content/drive/MyDrive/face.mat')\n",
    "# print(face_mat.keys())\n",
    "face_data = face_mat['X']\n",
    "# print(face_data)\n",
    "\n",
    "face_label = face_mat['l']\n",
    "# print(face_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTFkPR5wRdaB"
   },
   "source": [
    "3. Split data into training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "aborted",
     "timestamp": 1730519368601,
     "user": {
      "displayName": "신지환",
      "userId": "04463358478114875480"
     },
     "user_tz": -540
    },
    "id": "tKQxpM8UP8tI"
   },
   "outputs": [],
   "source": [
    "# Reshape face_label to be a 1D array\n",
    "face_label = face_label.flatten()\n",
    "print(\"Reshaped face_label shape:\", face_label.shape)\n",
    "\n",
    "# Unique identities in the dataset\n",
    "unique_identities = np.unique(face_label)\n",
    "\n",
    "# Lists to hold the train and test splits\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "# Loop over each unique identity\n",
    "for identity in unique_identities:\n",
    "    # Get all data and labels where the label matches the current identity\n",
    "    identity_data = face_data[:, face_label == identity].T\n",
    "    identity_labels = face_label[face_label == identity]\n",
    "\n",
    "    # Use train_test_split to split 8 images for training, 2 for testing\n",
    "    identity_X_train, identity_X_test, identity_y_train, identity_y_test = ms.train_test_split(\n",
    "        identity_data, identity_labels, train_size=8, test_size=2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Append to respective lists\n",
    "    X_train.append(identity_X_train)\n",
    "    y_train.append(identity_y_train)\n",
    "    X_test.append(identity_X_test)\n",
    "    y_test.append(identity_y_test)\n",
    "\n",
    "# Concatenate all individual arrays into final train and test datasets\n",
    "X_train = np.concatenate(X_train)\n",
    "y_train = np.concatenate(y_train)\n",
    "X_test = np.concatenate(X_test)\n",
    "y_test = np.concatenate(y_test)\n",
    "\n",
    "# Print the shapes to confirm the split\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pV6DI-KdRrr3"
   },
   "source": [
    "4. Define pca, lda function for M_pca and M_lda\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "aborted",
     "timestamp": 1730519368602,
     "user": {
      "displayName": "신지환",
      "userId": "04463358478114875480"
     },
     "user_tz": -540
    },
    "id": "_3BnQDhWPUbz"
   },
   "outputs": [],
   "source": [
    "# Step 1: Perform PCA for dimensionality reduction\n",
    "def apply_pca(X_train, X_test, n_components):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    return X_train_pca, X_test_pca, pca\n",
    "\n",
    "# Step 2: Perform LDA for class separability\n",
    "def apply_lda(X_train, X_test, y_train, n_components):\n",
    "    lda = LDA(n_components=n_components)\n",
    "    X_train_lda = lda.fit_transform(X_train, y_train)\n",
    "    X_test_lda = lda.transform(X_test)\n",
    "    return X_train_lda, X_test_lda, lda\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "aborted",
     "timestamp": 1730519368603,
     "user": {
      "displayName": "신지환",
      "userId": "04463358478114875480"
     },
     "user_tz": -540
    },
    "id": "JR5Az2SRR72P"
   },
   "outputs": [],
   "source": [
    "# Define PCA and LDA parameters\n",
    "M_pca = 100  # Adjust based on experimentation\n",
    "M_lda = 51   # Maximum LDA components (C - 1, where C is the number of classes)\n",
    "\n",
    "# Apply PCA\n",
    "X_train_pca, X_test_pca, pca = apply_pca(X_train, X_test, M_pca)\n",
    "print(\"PCA-reduced X_train shape:\", X_train_pca.shape)\n",
    "print(\"PCA-reduced X_test shape:\", X_test_pca.shape)\n",
    "\n",
    "# Apply LDA\n",
    "X_train_lda, X_test_lda, lda = apply_lda(X_train_pca, X_test_pca, y_train, M_lda)\n",
    "print(\"LDA-reduced X_train shape:\", X_train_lda.shape)\n",
    "print(\"LDA-reduced X_test shape:\", X_test_lda.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvaLlD15cndc"
   },
   "source": [
    "5. Calculate accuracy using NN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "aborted",
     "timestamp": 1730519368603,
     "user": {
      "displayName": "신지환",
      "userId": "04463358478114875480"
     },
     "user_tz": -540
    },
    "id": "fXEdUic4Ve0D"
   },
   "outputs": [],
   "source": [
    "# Step 3: Train and test the NN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train_lda, y_train)\n",
    "y_pred = knn.predict(X_test_lda)\n",
    "\n",
    "# Step 4: Evaluate model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Recognition accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XvjT_ehpcv49"
   },
   "source": [
    "6. Calculate confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "aborted",
     "timestamp": 1730519368604,
     "user": {
      "displayName": "신지환",
      "userId": "04463358478114875480"
     },
     "user_tz": -540
    },
    "id": "1fMQEC0sVlD-"
   },
   "outputs": [],
   "source": [
    "# Step 5: Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8l2KDIKcyvU"
   },
   "source": [
    "7. Calculate rank of scatter matrices\n",
    "8. Example of success and fail cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 39,
     "status": "aborted",
     "timestamp": 1730519368605,
     "user": {
      "displayName": "신지환",
      "userId": "04463358478114875480"
     },
     "user_tz": -540
    },
    "id": "hWez8NK2VrWi"
   },
   "outputs": [],
   "source": [
    "# Step 6: Calculate ranks of scatter matrices in LDA\n",
    "def calculate_lda_scatter_matrices(X, y):\n",
    "    n_features = X.shape[1]\n",
    "    classes = np.unique(y)\n",
    "    mean_overall = np.mean(X, axis=0)\n",
    "\n",
    "    # Initialize within-class scatter matrix (Sw) and between-class scatter matrix (Sb)\n",
    "    Sw = np.zeros((n_features, n_features))\n",
    "    Sb = np.zeros((n_features, n_features))\n",
    "\n",
    "    for cls in classes:\n",
    "        X_c = X[y == cls]\n",
    "        mean_class = np.mean(X_c, axis=0)\n",
    "\n",
    "        # Within-class scatter matrix\n",
    "        Sw += np.dot((X_c - mean_class).T, (X_c - mean_class))\n",
    "\n",
    "        # Between-class scatter matrix\n",
    "        n_c = X_c.shape[0]\n",
    "        mean_diff = (mean_class - mean_overall).reshape(n_features, 1)\n",
    "        Sb += n_c * mean_diff.dot(mean_diff.T)\n",
    "\n",
    "    return Sw, Sb\n",
    "\n",
    "# Calculate scatter matrices and their ranks\n",
    "Sw, Sb = calculate_lda_scatter_matrices(X_train_pca, y_train)\n",
    "rank_Sw = np.linalg.matrix_rank(Sw)\n",
    "rank_Sb = np.linalg.matrix_rank(Sb)\n",
    "print(\"Rank of within-class scatter matrix (Sw):\", rank_Sw)\n",
    "print(\"Rank of between-class scatter matrix (Sb):\", rank_Sb)\n",
    "\n",
    "# Example success and failure cases\n",
    "success_cases = np.where(y_pred == y_test)[0]\n",
    "failure_cases = np.where(y_pred != y_test)[0]\n",
    "\n",
    "print(\"Example of correct classifications:\", success_cases[:])\n",
    "print(\"Example of incorrect classifications:\", failure_cases[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52yG9ULgRHaR"
   },
   "source": [
    "## PCA-LDA Ensemble\n",
    "\n",
    "Show, measure and discuss the results, including:\n",
    "\n",
    "* Randomisation in features space\n",
    "* Randomisation on data samples (i.e. bagging)\n",
    "* Number of base models, the randomness parameter\n",
    "* Error of the committee machine vs. Average error of individual models\n",
    "* Fusion rules\n",
    "* Recognition accuracy and confusion matrix\n",
    "\n",
    "Observe and discuss the above by varying the parameter values/architectures you used. Give insights and reasons behind all your answers.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OnxaRjGc-LZ"
   },
   "source": [
    "1. Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "aborted",
     "timestamp": 1730519368606,
     "user": {
      "displayName": "신지환",
      "userId": "04463358478114875480"
     },
     "user_tz": -540
    },
    "id": "SDr6wjTpRHIk"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tScp1Yt2dB1u"
   },
   "source": [
    "2. Create randomized models to calculate average error and accuracy of individual models\n",
    " - randomized features\n",
    " - randomized samples (i.e. bagging)\n",
    " - set number of models, randomness parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "aborted",
     "timestamp": 1730519368606,
     "user": {
      "displayName": "신지환",
      "userId": "04463358478114875480"
     },
     "user_tz": -540
    },
    "id": "MaJ_6d_xPUM8"
   },
   "outputs": [],
   "source": [
    "# Ensemble parameters\n",
    "num_models = 10                # Number of base models\n",
    "randomness_param = 0.7         # Randomness parameter (fraction of features and samples)\n",
    "\n",
    "# Lists to hold results\n",
    "individual_accuracies = []\n",
    "individual_predictions = []\n",
    "\n",
    "# Step 1: Define function for randomized feature space (PCA) and bagging (random data samples)\n",
    "def create_randomized_model(X_train, y_train, X_test, y_test, randomness_param, M_pca, M_lda):\n",
    "    # Randomly sample features for PCA\n",
    "    num_features = int(X_train.shape[1] * randomness_param)\n",
    "    selected_features = np.random.choice(X_train.shape[1], num_features, replace=False)\n",
    "\n",
    "    # Randomly sample data points for bagging\n",
    "    num_samples = int(X_train.shape[0] * randomness_param)\n",
    "    sample_indices = np.random.choice(X_train.shape[0], num_samples, replace=True)\n",
    "\n",
    "    X_train_sample = X_train[sample_indices][:, selected_features]\n",
    "    y_train_sample = y_train[sample_indices]\n",
    "    X_test_sample = X_test[:, selected_features]\n",
    "\n",
    "    # Apply PCA and LDA\n",
    "    pca = PCA(n_components=min(M_pca, num_features))\n",
    "    X_train_pca = pca.fit_transform(X_train_sample)\n",
    "    X_test_pca = pca.transform(X_test_sample)\n",
    "\n",
    "    lda = LDA(n_components=min(M_lda, len(np.unique(y_train_sample)) - 1))\n",
    "    X_train_lda = lda.fit_transform(X_train_pca, y_train_sample)\n",
    "    X_test_lda = lda.transform(X_test_pca)\n",
    "\n",
    "    # Train NN classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=1)\n",
    "    knn.fit(X_train_lda, y_train_sample)\n",
    "    y_pred = knn.predict(X_test_lda)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return y_pred, accuracy\n",
    "\n",
    "# Step 2: Train ensemble of PCA-LDA models with randomization\n",
    "for i in range(num_models):\n",
    "    y_pred, accuracy = create_randomized_model(X_train, y_train, X_test, y_test, randomness_param, M_pca=100, M_lda=51)\n",
    "    individual_predictions.append(y_pred)\n",
    "    individual_accuracies.append(accuracy)\n",
    "\n",
    "# Step 3: Evaluate individual model performance\n",
    "average_individual_accuracy = np.mean(individual_accuracies)\n",
    "individual_errors = [1 - acc for acc in individual_accuracies]\n",
    "average_individual_error = np.mean(individual_errors)\n",
    "\n",
    "print(f\"Average individual error: {average_individual_error:.4f}\")\n",
    "print(f\"Average accuracy of individual models: {average_individual_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1atO5VxdnHo"
   },
   "source": [
    "3. Error and accuracy of committee machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "aborted",
     "timestamp": 1730519368607,
     "user": {
      "displayName": "신지환",
      "userId": "04463358478114875480"
     },
     "user_tz": -540
    },
    "id": "YCLGt4kfU103"
   },
   "outputs": [],
   "source": [
    "# Step 4: Implement committee fusion rules\n",
    "# Majority voting (default fusion rule)\n",
    "def majority_vote(predictions):\n",
    "    predictions = np.array(predictions)\n",
    "    majority_vote_predictions = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=predictions)\n",
    "    return majority_vote_predictions\n",
    "\n",
    "# Committee prediction\n",
    "committee_predictions = majority_vote(individual_predictions)\n",
    "committee_accuracy = accuracy_score(y_test, committee_predictions)\n",
    "committee_error = 1 - committee_accuracy\n",
    "\n",
    "print(f\"Committee error: {committee_error:.4f}\")\n",
    "print(f\"Committee (majority vote) accuracy: {committee_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIvux8hxd1Ah"
   },
   "source": [
    "4. Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 35,
     "status": "aborted",
     "timestamp": 1730519368607,
     "user": {
      "displayName": "신지환",
      "userId": "04463358478114875480"
     },
     "user_tz": -540
    },
    "id": "8eFJR_btU4zL"
   },
   "outputs": [],
   "source": [
    "# Step 5: Confusion matrix for committee predictions\n",
    "conf_matrix = confusion_matrix(y_test, committee_predictions)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix - Committee Model\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO/MTFoiw1Qx0TAut+qDB8n",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cs485_cw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
